{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdJ61Dv7YlWU"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Climate_Change_Prediction_DCGAN(iter30).ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/github/junhuk1113/AIAD_weather/blob/master/Climate_Change_Prediction_DCGAN(iter30).ipynb\n",
        "\"\"\"\n",
        "\n",
        "!pip install catboost\n",
        "\n",
        "#GENERAL\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import time\n",
        "#PATH PROCESS\n",
        "import os\n",
        "import os.path\n",
        "from pathlib import Path\n",
        "import glob\n",
        "#IMAGE PROCESS\n",
        "from PIL import Image\n",
        "from keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import cv2\n",
        "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
        "import imageio\n",
        "from IPython.display import Image\n",
        "import matplotlib.image as mpimg\n",
        "from skimage.transform import resize\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib import cm\n",
        "import zipfile\n",
        "from io import BytesIO\n",
        "from nibabel import FileHolder\n",
        "from nibabel.analyze import AnalyzeImage\n",
        "import PIL\n",
        "from IPython import display\n",
        "#SCALER & TRANSFORMATION\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import regularizers\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "#ACCURACY CONTROL\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_auc_score, roc_curve\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "#OPTIMIZER\n",
        "from keras.optimizers import RMSprop,Adam,Optimizer,Optimizer, SGD\n",
        "#MODEL LAYERS\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization,MaxPooling2D,BatchNormalization,\\\n",
        "                        Permute, TimeDistributed, Bidirectional,GRU, SimpleRNN,\\\n",
        "LSTM, GlobalAveragePooling2D, SeparableConv2D, ZeroPadding2D, Convolution2D, ZeroPadding2D,Reshape, Conv2DTranspose, LeakyReLU,\\\n",
        "ConvLSTM2D,Conv3D\n",
        "from keras import models\n",
        "from keras import layers\n",
        "import tensorflow as tf\n",
        "from keras.applications import VGG16,VGG19,inception_v3\n",
        "from keras import backend as K\n",
        "from keras.utils import plot_model\n",
        "from keras.datasets import mnist\n",
        "import keras\n",
        "#SKLEARN CLASSIFIER\n",
        "from xgboost import XGBClassifier, XGBRegressor\n",
        "from lightgbm import LGBMClassifier, LGBMRegressor\n",
        "from catboost import CatBoostClassifier, CatBoostRegressor\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import RidgeCV\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.linear_model import ElasticNetCV\n",
        "#IGNORING WARNINGS\n",
        "from warnings import filterwarnings\n",
        "filterwarnings(\"ignore\",category=DeprecationWarning)\n",
        "filterwarnings(\"ignore\", category=FutureWarning)\n",
        "filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "Carbon_Video_Set = \"/content/drive/MyDrive/PBL_Shared_Data/202101cldpng_unpacked/unpacked_resize.mp4\"\n",
        "#Ice_Video_Set = \".mp4\"\n",
        "\n",
        "Video_IMG_List = []\n",
        "\n",
        "Capture_Video = cv2.VideoCapture(Carbon_Video_Set)\n",
        "\n",
        "while Capture_Video.isOpened():\n",
        "\n",
        "    ret,frame = Capture_Video.read()\n",
        "\n",
        "    if ret != True:\n",
        "        break\n",
        "\n",
        "    if Capture_Video.isOpened():\n",
        "        Transformation_IMG = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
        "        Resize_IMG = cv2.resize(Transformation_IMG,(180,180))\n",
        "        (Resize_IMG - 127.5) / 127.5\n",
        "        Video_IMG_List.append(Resize_IMG)\n",
        "\n",
        "\n",
        "Capture_Video.release()\n",
        "\n",
        "print(np.shape(np.array(Video_IMG_List)))\n",
        "\n",
        "Main_Array = np.array(Video_IMG_List)\n",
        "\n",
        "print(Main_Array.shape)\n",
        "\n",
        "def simple_vision(image):\n",
        "\n",
        "    figure = plt.figure(figsize=(10,10))\n",
        "\n",
        "    plt.xlabel(image.shape)\n",
        "    plt.ylabel(image.size)\n",
        "    plt.imshow(image)\n",
        "\n",
        "def threshold_vision(image):\n",
        "\n",
        "    figure = plt.figure(figsize=(10,10))\n",
        "\n",
        "    _,Threshold_IMG = cv2.threshold(image,130,255,cv2.THRESH_BINARY)\n",
        "\n",
        "    plt.xlabel(Threshold_IMG.shape)\n",
        "    plt.ylabel(Threshold_IMG.size)\n",
        "    plt.imshow(Threshold_IMG)\n",
        "\n",
        "def Red_inRange_vision(image,lower_red,upper_red,lower_red_two,upper_red_two):\n",
        "\n",
        "    figure = plt.figure(figsize=(10,10))\n",
        "    mask_kernel_shape = np.ones((5,5),dtype=\"uint8\")\n",
        "\n",
        "    img_hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
        "\n",
        "    Mask_Img_One = cv2.inRange(img_hsv,lower_red,upper_red)\n",
        "    Dilate_Img_One = cv2.dilate(Mask_Img_One,mask_kernel_shape)\n",
        "\n",
        "    Mask_Img_Two = cv2.inRange(img_hsv,lower_red_two,upper_red_two)\n",
        "    Dilate_Img_Two = cv2.dilate(Mask_Img_Two,mask_kernel_shape)\n",
        "\n",
        "    Main_Mask = Dilate_Img_One+Dilate_Img_Two\n",
        "\n",
        "    output_img = image.copy()\n",
        "    output_img[np.where(Main_Mask==0)] = 0\n",
        "    output_hsv = img_hsv.copy()\n",
        "    output_hsv[np.where(Main_Mask==0)] = 0\n",
        "\n",
        "\n",
        "    plt.xlabel(output_img.shape)\n",
        "    plt.ylabel(output_img.size)\n",
        "    plt.imshow(output_img)\n",
        "\n",
        "def White_inRange_vision(image,lower_white,upper_white):\n",
        "\n",
        "    figure = plt.figure(figsize=(10,10))\n",
        "    mask_kernel_shape = np.ones((5,5),dtype=\"uint8\")\n",
        "\n",
        "    img_hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
        "\n",
        "    Mask_Img = cv2.inRange(img_hsv,lower_white,upper_white)\n",
        "    Dilate_Img = cv2.dilate(Mask_Img,mask_kernel_shape)\n",
        "\n",
        "    image_bitwise = cv2.bitwise_and(image,image, mask = Dilate_Img)\n",
        "\n",
        "\n",
        "    plt.xlabel(image_bitwise.shape)\n",
        "    plt.ylabel(image_bitwise.size)\n",
        "    plt.imshow(image_bitwise)\n",
        "\n",
        "simple_vision(Main_Array[12])\n",
        "\n",
        "simple_vision(Main_Array[1])\n",
        "\n",
        "simple_vision(Main_Array[297])\n",
        "\n",
        "threshold_vision(Main_Array[26])\n",
        "\n",
        "threshold_vision(Main_Array[6])\n",
        "\n",
        "threshold_vision(Main_Array[16])\n",
        "\n",
        "lower_white = np.array([0,0,0], dtype=np.uint8)\n",
        "upper_white = np.array([0,0,255], dtype=np.uint8)\n",
        "\n",
        "White_inRange_vision(Main_Array[6],lower_white,upper_white)\n",
        "\n",
        "lower_white = np.array([0,0,0], dtype=np.uint8)\n",
        "upper_white = np.array([0,0,255], dtype=np.uint8)\n",
        "\n",
        "White_inRange_vision(Main_Array[56],lower_white,upper_white)\n",
        "\n",
        "lower_white = np.array([0,0,0], dtype=np.uint8)\n",
        "upper_white = np.array([0,0,255], dtype=np.uint8)\n",
        "\n",
        "White_inRange_vision(Main_Array[60],lower_white,upper_white)\n",
        "\n",
        "lower_white = np.array([0,0,0], dtype=np.uint8)\n",
        "upper_white = np.array([0,0,255], dtype=np.uint8)\n",
        "\n",
        "White_inRange_vision(Main_Array[1],lower_white,upper_white)\n",
        "\n",
        "lower_white = np.array([0,0,0], dtype=np.uint8)\n",
        "upper_white = np.array([0,0,255], dtype=np.uint8)\n",
        "\n",
        "White_inRange_vision(Main_Array[100],lower_white,upper_white)\n",
        "\n",
        "lower_white = np.array([0,0,0], dtype=np.uint8)\n",
        "upper_white = np.array([0,0,255], dtype=np.uint8)\n",
        "\n",
        "White_inRange_vision(Main_Array[23],lower_white,upper_white)\n",
        "\n",
        "figure,axis = plt.subplots(5,5,figsize=(10,10))\n",
        "\n",
        "for indexing,operations in enumerate(axis.flat):\n",
        "\n",
        "    IMG_Picking = Main_Array[indexing]\n",
        "    operations.set_xlabel(IMG_Picking.shape)\n",
        "    operations.set_ylabel(IMG_Picking.size)\n",
        "    operations.imshow(IMG_Picking)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "iterations = 60\n",
        "vector_noise_shape = 180\n",
        "count_example = 16\n",
        "batch_size = 24\n",
        "count_buffer = 60000\n",
        "\n",
        "seed = tf.random.normal([count_example,vector_noise_shape])\n",
        "\n",
        "print(Main_Array.shape)\n",
        "\n",
        "Train_Data = tf.data.Dataset.from_tensor_slices(Main_Array).shuffle(count_buffer).batch(batch_size)\n",
        "\n",
        "print(Train_Data)\n",
        "\n",
        "def Generator_Model():\n",
        "\n",
        "\n",
        "    Model = Sequential()\n",
        "    #\n",
        "    Model.add(Dense(90*90*128,use_bias=False,input_shape=(180,)))\n",
        "    Model.add(BatchNormalization())\n",
        "    Model.add(LeakyReLU())\n",
        "    #\n",
        "    Model.add(Reshape((90,90,128)))\n",
        "    #\n",
        "    Model.add(Conv2DTranspose(128,(3,3),padding=\"same\",use_bias=False))\n",
        "    Model.add(BatchNormalization())\n",
        "    Model.add(LeakyReLU())\n",
        "\n",
        "    Model.add(Conv2DTranspose(64, (3,3), strides=(2,2), padding='same', use_bias=False))\n",
        "    Model.add(BatchNormalization())\n",
        "    Model.add(LeakyReLU())\n",
        "    #\n",
        "    Model.add(Conv2DTranspose(3,(3,3),padding=\"same\",use_bias=False,activation=\"tanh\"))\n",
        "\n",
        "\n",
        "    return Model\n",
        "\n",
        "Generator = Generator_Model()\n",
        "\n",
        "def Discriminator_Model():\n",
        "\n",
        "    Model = Sequential()\n",
        "\n",
        "    Model.add(Conv2D(64,(3,3),padding=\"same\",input_shape=[180,180,3]))\n",
        "    Model.add(Dropout(0.3))\n",
        "    Model.add(LeakyReLU())\n",
        "\n",
        "\n",
        "    Model.add(Conv2D(128,(3,3),padding=\"same\"))\n",
        "    Model.add(Dropout(0.3))\n",
        "    Model.add(LeakyReLU())\n",
        "\n",
        "    Model.add(Flatten())\n",
        "    Model.add(Dense(1))\n",
        "\n",
        "    return Model\n",
        "\n",
        "Discriminator = Discriminator_Model()\n",
        "\n",
        "Loss_Function = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def Discriminator_Loss(real_output,fake_output):\n",
        "\n",
        "    real_loss = Loss_Function(tf.ones_like(real_output),real_output)\n",
        "    fake_loss = Loss_Function(tf.zeros_like(fake_output),fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "def Generator_Loss(fake_output):\n",
        "\n",
        "    return Loss_Function(tf.ones_like(fake_output),fake_output)\n",
        "\n",
        "Generator_Optimizer = RMSprop(lr=0.0001,clipvalue=1.0,decay=1e-8)\n",
        "Discriminator_Optimizer = RMSprop(lr=0.0001,clipvalue=1.0,decay=1e-8)\n",
        "\n",
        "def generate_and_save_function(Model, epoch, test_input):\n",
        "\n",
        "    predictions = Model(test_input, training=False)\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "\n",
        "    for i in range(predictions.shape[0]):\n",
        "        plt.subplot(5, 5, i+1)\n",
        "        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5)\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.savefig('images_epoch_{:04d}.png'.format(epoch))\n",
        "    plt.show()\n",
        "\n",
        "def action_function(images):\n",
        "\n",
        "    random_noise_vector = tf.random.normal([batch_size,vector_noise_shape])\n",
        "\n",
        "    with tf.GradientTape() as Generator_Tape,tf.GradientTape() as Discriminator_Tape:\n",
        "        Generator_Fake_Images = Generator(random_noise_vector,training=False)\n",
        "        real_output = Discriminator(images,training=True)\n",
        "        fake_output = Discriminator(Generator_Fake_Images,training=True)\n",
        "        Generator_Loss_Output = Generator_Loss(fake_output)\n",
        "        Discriminator_Loss_Output = Discriminator_Loss(real_output,fake_output)\n",
        "\n",
        "    Generator_Gradients = Generator_Tape.gradient(Generator_Loss_Output,Generator.trainable_variables)\n",
        "    Discriminator_Gradients = Discriminator_Tape.gradient(Discriminator_Loss_Output,Discriminator.trainable_variables)\n",
        "\n",
        "    Generator_Optimizer.apply_gradients(zip(Generator_Gradients,Generator.trainable_variables))\n",
        "    Discriminator_Optimizer.apply_gradients(zip(Discriminator_Gradients,Discriminator.trainable_variables))\n",
        "\n",
        "def train_function(images_data,iterations):\n",
        "    for epoch in range(iterations):\n",
        "        start = time.time()\n",
        "        for batching in images_data:\n",
        "            action_function(batching)\n",
        "\n",
        "        display.clear_output(wait=True)\n",
        "        generate_and_save_function(Generator,epoch+1,seed)\n",
        "\n",
        "    display.clear_output(wait=True)\n",
        "    generate_and_save_function(Generator,epoch,seed)\n",
        "\n",
        "train_function(Train_Data,iterations)\n",
        "\n",
        "Predict_Random_Noise = tf.random.normal(shape=[60,vector_noise_shape])\n",
        "\n",
        "Generator_Prediction = Generator(Predict_Random_Noise)\n",
        "\n",
        "figure,axis = plt.subplots(3,3,figsize=(12,12))\n",
        "\n",
        "for i,ax in enumerate(axis.flat):\n",
        "    Image_Picking = Generator_Prediction[i]\n",
        "    ax.imshow(Image_Picking,cmap=\"gray\")\n",
        "    ax.set_xlabel(Image_Picking.shape)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "figure = plt.figure(figsize=(10,10))\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(Generator_Prediction[25])\n",
        "plt.show()\n",
        "\n",
        "figure = plt.figure(figsize=(10,10))\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(Generator_Prediction[55])\n",
        "plt.show()\n",
        "\n",
        "figure = plt.figure(figsize=(10,10))\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(Generator_Prediction[59])\n",
        "plt.show()\n",
        "\n",
        "figure = plt.figure(figsize=(10,10))\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(Generator_Prediction[9])\n",
        "plt.show()\n",
        "\n",
        "figure = plt.figure(figsize=(10,10))\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(Generator_Prediction[1])\n",
        "plt.show()\n",
        "\n",
        "print(Generator_Prediction[1].dtype)\n",
        "print(type(Generator_Prediction[1]))\n",
        "\n",
        "Layer_Output = [layer.output for layer in Generator.layers[:8]]\n",
        "activation_model = models.Model(inputs=Generator.input,outputs=Layer_Output)\n",
        "\n",
        "activations = activation_model.predict(seed)\n",
        "\n",
        "first_layer_act = activations[0]\n",
        "print(first_layer_act.shape)\n",
        "\n",
        "plt.matshow(first_layer_act[:10,:10],cmap=\"viridis\")\n",
        "\n",
        "plt.matshow(first_layer_act[:1,:10],cmap=\"viridis\")\n",
        "\n",
        "Generator.save(\"Generator.h5\")\n",
        "Discriminator.save(\"Discriminator.h5\")\n",
        "\n"
      ]
    }
  ]
}