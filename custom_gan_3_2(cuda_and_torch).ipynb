{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junhuk1113/AIAD_weather/blob/master/custom_gan_3_2(cuda_and_torch).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QPcLl7CSVcIN"
      },
      "outputs": [],
      "source": [
        "#import library\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from PIL import Image as Img\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "from IPython.display import Image\n",
        "\n",
        "from skimage import io, transform\n",
        "import imageio\n",
        "import datetime\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "#import keras\n",
        "\n",
        "import keras \n",
        "from keras import Input\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras import Model, layers\n",
        "from keras.callbacks import *\n",
        "from keras.models import load_model, model_from_json\n",
        "from keras.layers import Dense, Reshape, LeakyReLU, Conv2D, Conv2DTranspose, Flatten, Dropout\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#torch and cuda import\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.utils import save_image\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "call_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if call_cuda else \"cpu\")"
      ],
      "metadata": {
        "id": "8CxOfQtOVwnv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test on torch bring GPU\n",
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KFn-6bQNV8aM",
        "outputId": "2df42041-3879-45ab-fd1c-b9a2f9b3936b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fXPtjqU-an8",
        "outputId": "7d10f1c0-c45b-47e5-a543-c2192ed5f79f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = 64\n",
        "DATA_DIR = '/content/drive/MyDrive/PBL_Shared_Data/202101cldcr200re64new.npy'\n",
        "X_train = np.load(DATA_DIR)\n",
        "print(f\"Shape of training data: {X_train.shape}\")\n",
        "print(f\"Data type: {type(X_train)}\")\n",
        "\n",
        "print(type(X_train[0][0][0][0]))\n",
        "\n",
        "data = X_train.astype(np.float64)\n",
        "data = 255 * data\n",
        "img = data.astype(np.uint8)\n",
        "X_train = img\n",
        "\n",
        "random_image = random.randint(0, len(X_train))\n",
        "plt.imshow(X_train[random_image,0],cmap='gray',vmin=0,vmax=255)\n",
        "plt.title(f\"Training example #{random_image}\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "B0sEs4OqkDZx",
        "outputId": "7d322a68-90e5-4682-e89f-c696b054ad7a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of training data: (31, 24, 64, 64)\n",
            "Data type: <class 'numpy.ndarray'>\n",
            "<class 'numpy.uint8'>\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "USE_CUDA = torch.cuda.is_available()\n",
        "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
        "print(\"사용하는 Device : \", DEVICE)\n",
        "\n",
        "current_time = datetime.datetime.now() + datetime.timedelta(hours= 9)\n",
        "current_time = current_time.strftime('%Y-%m-%d-%H:%M')\n",
        "\n",
        "saved_loc = os.path.join('/content/drive/MyDrive/GAN_Result', current_time)\n",
        "if os.path.exists(saved_loc):\n",
        "    shutil.rmtree(saved_loc)\n",
        "os.mkdir(saved_loc)\n",
        "\n",
        "print(\"저장 위치: \", saved_loc)\n",
        "\n",
        "image_loc = os.path.join(saved_loc, \"images\")\n",
        "os.mkdir(image_loc)\n",
        "\n",
        "writer = SummaryWriter(saved_loc)\n",
        "\n",
        "EPOCHS = 32\n",
        "BATCH_SIZE = 200"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tLHd-wiXuPm",
        "outputId": "5e072373-be7b-4461-f285-058504fcc5eb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "사용하는 Device :  cuda\n",
            "저장 위치:  /content/drive/MyDrive/GAN_Result/2022-10-11-20:53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = X_train.astype(np.float64)\n",
        "data = 255 * data\n",
        "X_train = data.astype(np.uint8)"
      ],
      "metadata": {
        "id": "J6U4DavAki_P"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(X_train[0][0][0][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iat4AOSVjPEq",
        "outputId": "217d2d99-8797-478c-ea37-44c54173d837"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.uint8'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Class Define CloudDataset1\n",
        "class CloudDataset1(Dataset):\n",
        "  def __init__(self, X, y):\n",
        "        'Initialization'\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "  def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.X)\n",
        "  def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample\n",
        "        image = self.X[index]\n",
        "        X = self.transform(image)\n",
        "        return X\n",
        "        transform = T.Compose([\n",
        "        T.ToPILImage(),\n",
        "        T.Resize(image_size),\n",
        "        T.ToTensor()])\n",
        "# Transformer code\n",
        "transform = T.Compose([\n",
        "        T.ToPILImage(),\n",
        "        T.Resize(image_size),\n",
        "        T.ToTensor()])"
      ],
      "metadata": {
        "id": "mrC35PRJ_Nmi"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_images(images, nmax=64):\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    ax.set_xticks([]); ax.set_yticks([])\n",
        "    ax.imshow(make_grid((images.detach()[:nmax]), nrow=8).permute(1, 2, 0))\n",
        "def show_batch(dl, nmax=64):\n",
        "    for images in dl:\n",
        "        show_images(images, nmax)\n",
        "        break"
      ],
      "metadata": {
        "id": "DMbG5Seq0R8T"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Class Define CloudDataset2\n",
        "class CloudDataset2(Dataset):\n",
        "    'Characterizes a dataset for PyTorch'\n",
        "    def __init__(self, ims):\n",
        "        'Initialization'\n",
        "        self.ims = ims\n",
        "    def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.ims)\n",
        "    def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample\n",
        "        image = self.ims[index]\n",
        "        X = self.transform(image)\n",
        "        return X\n",
        "        \n",
        "transform = T.Compose([\n",
        "      T.ToPILImage(),\n",
        "      T.CenterCrop(0.75 * 64),\n",
        "      T.Resize(image_size),\n",
        "      T.RandomResizedCrop(image_size),\n",
        "      T.RandomHorizontalFlip(),\n",
        "      T.ToTensor()])\n",
        "    \n",
        "batch_size = 64\n",
        "cropped_dataset = CloudDataset2(ims=X_train)\n",
        "train_dl = DataLoader(cropped_dataset, batch_size, shuffle=True, num_workers=3, pin_memory=True)\n",
        "#show_batch(train_dl)"
      ],
      "metadata": {
        "id": "ryn0cYf3uFXe"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample, label = next(iter(trainloader))\n",
        "\n",
        "# show grid image\n",
        "def imshow_grid(img):\n",
        "    img = torchvision.utils.make_grid(img.cpu().detach())\n",
        "    img = (img + 1) / 2\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    ax = plt.gca()\n",
        "    ax.axes.xaxis.set_visible(False)\n",
        "    ax.axes.yaxis.set_visible(False)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "imshow_grid(sample[0:8])\n",
        "\n",
        "# Loading trainset, testset and trainloader, testloader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "gq-heA2yFcaj",
        "outputId": "4da4574f-3587-4ea3-df41-c6ecd8160139"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-b987374ba67d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCloudDataset2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# show grid image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mimshow_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'type' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Discriminator class\n",
        "class Dis_model(nn.Module):\n",
        "    def __init__(self, image_size, hidden_space):\n",
        "        super(Dis_model, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Linear(image_size, hidden_space),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_space, hidden_space),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_space, 1),\n",
        "            nn.Sigmoid())\n",
        "    \n",
        "    def forward(self, input_x):\n",
        "        x = self.features(input_x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "KSlXrbKe06nX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generator class\n",
        "class Gen_model(nn.Module):\n",
        "    def __init__(self, latent_space, hidden_space, image_size):\n",
        "        super(Gen_model, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Linear(latent_space, hidden_space),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_space, hidden_space),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_space, image_size),\n",
        "            nn.Tanh())\n",
        "        \n",
        "    def forward(self, input_x):\n",
        "        x = self.features(input_x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "I9RmGzRH0Nbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "im_size = 784\n",
        "hidden_size = 256\n",
        "latent_size = 100\n",
        "\n",
        "Dis_net = Dis_model(image_size = im_size, hidden_space = hidden_size).to(DEVICE)\n",
        "Gen_net = Gen_model(image_size = im_size, hidden_space = hidden_size, latent_space = latent_size).to(DEVICE)\n",
        "\n",
        "d_optimizer = optim.Adam(Dis_net.parameters(), lr = 0.0002)\n",
        "g_optimizer = optim.Adam(Gen_net.parameters(), lr = 0.0002)\n",
        "\n",
        "fixed_noise = torch.randn(10, latent_size, 1, 1, device = DEVICE)"
      ],
      "metadata": {
        "id": "J-7ROVzJ0woo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train model,train version\n",
        "\n",
        "def train(generator, discriminator, train_loader, optimizer_d, optimizer_g):\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "\n",
        "    D_losses = []\n",
        "    G_losses = []\n",
        "\n",
        "    for data, target in train_loader:\n",
        "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "\n",
        "        #optimize discriminator\n",
        "\n",
        "        # initialize discriminator optimizer\n",
        "        optimizer_d.zero_grad()\n",
        "\n",
        "        # Make noise samples for discriminator update\n",
        "        noise_samples_d = torch.randn(BATCH_SIZE, latent_size).to(DEVICE)\n",
        "\n",
        "        # real loss\n",
        "        discri_value = discriminator(data.view(-1, 28*28))\n",
        "        loss_real = -1 * torch.log(discri_value) # gradient ascent\n",
        "\n",
        "        # fake loss\n",
        "        gene_value = discriminator(generator(noise_samples_d))\n",
        "        loss_fake = -1 * torch.log(1.0 - gene_value) # gradient ascent\n",
        "\n",
        "        # Final loss\n",
        "        loss_d = (loss_real + loss_fake).mean()\n",
        "\n",
        "        loss_d.backward()\n",
        "        optimizer_d.step()\n",
        "\n",
        "\n",
        "        #optimize generator\n",
        "\n",
        "        # initialize generator optimizer\n",
        "        optimizer_g.zero_grad()\n",
        "\n",
        "        # Make noise samples for generator update\n",
        "        noise_samples_g = torch.randn(BATCH_SIZE, latent_size).to(DEVICE)\n",
        "\n",
        "        # calculate loss\n",
        "        fake_value = discriminator(generator(noise_samples_g))\n",
        "        loss_generator = -1 * torch.log(fake_value).mean() # provide much stronger gradients early in learning.\n",
        "        \n",
        "        loss_generator.backward()\n",
        "        optimizer_g.step()\n",
        "\n",
        "    D_loss_epoch = torch.mean(torch.FloatTensor(D_losses))\n",
        "    G_loss_epoch = torch.mean(torch.FloatTensor(G_losses))\n",
        "    writer.add_scalars(\"Train\", {\"Average Discriminator loss per epoch\" : D_loss_epoch.item(),\n",
        "                                 \"Average Generator loss per epoch\" : G_loss_epoch.item()}, epoch)"
      ],
      "metadata": {
        "id": "KcDKyLnd1Ugg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in tqdm(range(EPOCHS)):\n",
        "    train(Gen_net, Dis_net, trainloader, d_optimizer, g_optimizer)\n",
        "\n",
        "    fixed_noise_generation = Gen_net(fixed_noise.view(-1, latent_size))\n",
        "    torchvision.utils.save_image(fixed_noise_generation.detach(),\n",
        "                                 '%s/fixed_noise_generation_epoch_%03d.png' % (image_loc, epoch), normalize = True)\n",
        "\n",
        "    if (epoch+1)%20 == 0:\n",
        "        print('epoch %i / %i' % (epoch+1, EPOCHS))\n",
        "        \n",
        "        noise_sam = torch.randn(16, latent_size).to(DEVICE)\n",
        "        imshow_grid(Gen_net(noise_sam).view(-1, 1, 28, 28))\n",
        "        print(\"\\n\")\n",
        "\n",
        "writer.close()\n"
      ],
      "metadata": {
        "id": "GOwRFwpB4wst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "class CustomDataset(Dataset): \n",
        "  def __init__(self):\n",
        "    self.x_data = [[73, 80, 75],\n",
        "                   [93, 88, 93],\n",
        "                   [89, 91, 90],\n",
        "                   [96, 98, 100],\n",
        "                   [73, 66, 70]]\n",
        "    self.y_data = [[152], [185], [180], [196], [142]]\n",
        "\n",
        "  # 총 데이터의 개수를 리턴\n",
        "  def __len__(self): \n",
        "    return len(self.x_data)\n",
        "\n",
        "  # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴\n",
        "  def __getitem__(self, idx): \n",
        "    x = torch.FloatTensor(self.x_data[idx])\n",
        "    y = torch.FloatTensor(self.y_data[idx])\n",
        "    return x, y\n",
        "dataset = CustomDataset()\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "model = torch.nn.Linear(3,1)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) \n",
        "nb_epochs = 20\n",
        "for epoch in range(nb_epochs + 1):\n",
        "  for batch_idx, samples in enumerate(dataloader):\n",
        "    # print(batch_idx)\n",
        "    # print(samples)\n",
        "    x_train, y_train = samples\n",
        "    # H(x) 계산\n",
        "    prediction = model(x_train)\n",
        "\n",
        "    # cost 계산\n",
        "    cost = F.mse_loss(prediction, y_train)\n",
        "\n",
        "    # cost로 H(x) 계산\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(\n",
        "        epoch, nb_epochs, batch_idx+1, len(dataloader),\n",
        "        cost.item()\n",
        "        ))\n",
        "Epoch\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "U7WsRKf_mG_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "image_size = 64\n",
        "DATA_DIR = '/content/drive/MyDrive/PBL_Shared_Data/202101cldcr200re64new.npy'\n",
        "X_train = np.load(DATA_DIR)\n",
        "print(f\"Shape of training data: {X_train.shape}\")\n",
        "print(f\"Data type: {type(X_train)}\")\n",
        "\n",
        "print(type(X_train[0][0][0][0]))\n",
        "\n",
        "#data = X_train.astype(np.float64)\n",
        "# = 255 * data\n",
        "#X_train = data.astype(np.uint8)\n",
        "random_image = random.randint(0, len(X_train))\n",
        "plt.imshow(X_train[random_image,0],cmap='gray',vmin=0,vmax=255)\n",
        "plt.title(f\"Training example #{random_image}\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "xHr8DFefmL7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "transformer = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "class SimpleCustomBatch:\n",
        "    def __init__(self, data):\n",
        "        transposed_data = list(zip(*data))\n",
        "        self.inp = torch.stack(transposed_data[0], 0)\n",
        "        self.tgt = torch.stack(transposed_data[1], 0)\n",
        "\n",
        "    # custom memory pinning method on custom type\n",
        "    def pin_memory(self):\n",
        "        self.inp = self.inp.pin_memory()\n",
        "        self.tgt = self.tgt.pin_memory()\n",
        "        return self\n",
        "\n",
        "def collate_wrapper(batch):\n",
        "    return SimpleCustomBatch(batch)\n",
        "\n",
        "inps = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\n",
        "tgts = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\n",
        "dataset = TensorDataset(inps, tgts)\n",
        "\n",
        "loader = DataLoader(dataset, batch_size=2, collate_fn=collate_wrapper,\n",
        "                    pin_memory=True)\n",
        "\n",
        "for batch_ndx, sample in enumerate(loader):\n",
        "    print(sample.inp.is_pinned())\n",
        "    print(sample.tgt.is_pinned())\n",
        "    \"\"\"\n",
        "    "
      ],
      "metadata": {
        "id": "2xHws-G91w0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "class SimpleCustomBatch:\n",
        "    def __init__(self, data):\n",
        "        transposed_data = list(zip(*data))\n",
        "        self.inp = torch.stack(transposed_data[0], 0)\n",
        "        self.tgt = torch.stack(transposed_data[1], 0)\n",
        "\n",
        "    # custom memory pinning method on custom type\n",
        "    def pin_memory(self):\n",
        "        self.inp = self.inp.pin_memory()\n",
        "        self.tgt = self.tgt.pin_memory()\n",
        "        return self\n",
        "\n",
        "def collate_wrapper(batch):\n",
        "    return SimpleCustomBatch(batch)\n",
        "\n",
        "input = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\n",
        "target = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\n",
        "dataset = CustomDataset(inps, tgts)\n",
        "#dataset = CustomDataset(inps, tgts)\n",
        "\n",
        "loader = DataLoader(dataset, batch_size=2, collate_fn=collate_wrapper,\n",
        "                    pin_memory=True)\n",
        "\n",
        "for batch_ndx, sample in enumerate(loader):\n",
        "    print(sample.inp.is_pinned())\n",
        "    print(sample.tgt.is_pinned())\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "h6xqx4hK11ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AaN8yK52VqQ6"
      }
    }
  ]
}